{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anneal function and agent related functions.\n"
      ],
      "metadata": {
        "id": "wE6uaDTmcbyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#libraries\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "uP7AomebeYlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7ed8d8-004b-4239-b18c-d899535617e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alive_progress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gmvWgOOZzf4",
        "outputId": "e9670f1b-cf66-4062-c3e6-398ec6198c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: alive_progress in /usr/local/lib/python3.10/dist-packages (3.1.4)\n",
            "Requirement already satisfied: about-time==4.2.1 in /usr/local/lib/python3.10/dist-packages (from alive_progress) (4.2.1)\n",
            "Requirement already satisfied: grapheme==0.6.0 in /usr/local/lib/python3.10/dist-packages (from alive_progress) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from alive_progress import alive_bar"
      ],
      "metadata": {
        "id": "XiBKg7mwZ2LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annealing\n",
        "Takes care of annealing, AKA decreasing the learning rate, if requested"
      ],
      "metadata": {
        "id": "lvUI6twxt2TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exponentially descrease the learning rate if toggled\n",
        "def anneal(anneal_lr, update_step, num_update, learning_rate):\n",
        "  if anneal_lr:\n",
        "    frac = 1.0 - (update_step - 1.0) / num_update\n",
        "    lrnow = frac * learning_rate\n",
        "    return lrnow\n",
        "  else:\n",
        "    return learning_rate\n",
        "\n"
      ],
      "metadata": {
        "id": "oZWixTSBcjx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating agent\n",
        "Takes care of updating agent relating lists"
      ],
      "metadata": {
        "id": "Znm3hkZhuAQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_agent(envs_v, obs_v, actions_v, logprobs_v, rewards_v,\\\n",
        "           dones_v, values_v, next_obs_v, next_done_v, agent_v,\\\n",
        "                 step_loop, device_v):\n",
        "  obs_v[step_loop] = next_obs_v\n",
        "  dones_v[step_loop] = next_done_v\n",
        "\n",
        "  # ALGO LOGIC: action logic\n",
        "  with torch.no_grad():\n",
        "      action, logprob, _, value = agent_v.get_action_and_value(next_obs_v)\n",
        "      values_v[step_loop] = value.flatten()\n",
        "  actions_v[step_loop] = action\n",
        "  logprobs_v[step_loop] = logprob\n",
        "\n",
        "  # TRY NOT TO MODIFY: execute the game and log data.\n",
        "  next_obs_v, reward, done, info_v = envs_v.step(action.cpu().numpy())\n",
        "  rewards_v[step_loop] = torch.tensor(reward).to(device_v).view(-1)\n",
        "  next_obs_v, next_done_v = torch.Tensor(next_obs_v).to(device_v), torch.Tensor(done).to(device_v)\n",
        "\n",
        "  return obs_v, actions_v, logprobs_v, rewards_v, dones_v, values_v, next_obs_v, next_done_v, info_v\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FE3tQsIMco_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAE\n",
        "General Advantage estimation, basically an algorithm to estimate the advantage function.\n",
        "\n",
        "The original paper about GAE can be found [here](https://arxiv.org/abs/1506.02438)\n"
      ],
      "metadata": {
        "id": "FiIkZ9oLuQsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GAE(gae_v, gae_lambda_v, gamma_v, agent_v,\\\n",
        "        values_v, dones_v, rewards_v, next_obs_v, next_done_v,\\\n",
        "        num_steps_v, device_v):\n",
        "  # bootstrap value if not done\n",
        "  with torch.no_grad():\n",
        "    next_value = agent_v.get_value(next_obs_v).reshape(1, -1)\n",
        "    if gae_v:\n",
        "        advantages_v = torch.zeros_like(rewards_v).to(device_v)\n",
        "        lastgaelam = 0\n",
        "        for t in reversed(range(num_steps_v)):\n",
        "            if t == num_steps_v - 1:\n",
        "                nextnonterminal = 1.0 - next_done_v\n",
        "                nextvalues = next_value\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - dones_v[t + 1]\n",
        "                nextvalues = values_v[t + 1]\n",
        "            delta = rewards_v[t] + gamma_v * nextvalues * nextnonterminal - values_v[t]\n",
        "            advantages_v[t] = lastgaelam =\\\n",
        "             delta + gamma_v * gae_lambda_v * nextnonterminal * lastgaelam\n",
        "        returns_v = advantages_v + values_v\n",
        "    else:\n",
        "        returns_v = torch.zeros_like(rewards_v).to(device_v)\n",
        "        for t in reversed(range(num_steps_v)):\n",
        "            if t == num_steps_v - 1:\n",
        "                nextnonterminal = 1.0 - next_done_v\n",
        "                next_return = next_value\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - dones_v[t + 1]\n",
        "                next_return = returns[t + 1]\n",
        "            returns_v[t] = rewards_v[t] + gamma_v * nextnonterminal * next_return\n",
        "        advantages_v = returns_v - values_v\n",
        "\n",
        "  return returns_v, advantages_v"
      ],
      "metadata": {
        "id": "OTcLBOg1uQJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO training loop\n",
        "Training loop with PPO algorithm.\n",
        "\n",
        "We added kl divergence, that, in a nutshell, it's a simple way to understand how aggressive the policy updates. Further details about the calculation can be found [here](http://joschu.net/blog/kl-approx.html).\n"
      ],
      "metadata": {
        "id": "2fIjEkislPYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PPO_train_agent(batch_size, update_epochs, minibatch_size, clip_coef, norm_adv, clip_vloss,\\\n",
        "                ent_coef, vf_coef, max_grad_norm, target_kl, \\\n",
        "                agent_v, optimizer_v,\\\n",
        "                b_obs_v, b_actions_v,b_logprobs_v, b_advantages_v, b_returns_v, b_values_v,\\\n",
        "                checkpoint = False):\n",
        "\n",
        "  #checkpoint is a bool that decides whether to enable or not checkpoint saving\n",
        "\n",
        "  # Optimizing the policy and value network\n",
        "  b_inds = np.arange(batch_size)\n",
        "  clipfracs = []\n",
        "  loss = 0\n",
        "  with alive_bar(update_epochs) as bar:\n",
        "    for epoch in range(update_epochs):\n",
        "        #print('Starting epoch {} of training'.format(epoch))\n",
        "        np.random.shuffle(b_inds)\n",
        "        #calculate ratio\n",
        "        for start in range(0, batch_size, minibatch_size):\n",
        "            end = start + minibatch_size\n",
        "            mb_inds = b_inds[start:end]\n",
        "\n",
        "            _, newlogprob, entropy, newvalue = agent_v.get_action_and_value(b_obs_v[mb_inds],\\\n",
        "                                                                            b_actions_v.long()[mb_inds])\n",
        "            logratio = newlogprob - b_logprobs_v[mb_inds]\n",
        "            ratio = logratio.exp()\n",
        "\n",
        "            # calculate approx_kl\n",
        "            with torch.no_grad():\n",
        "                old_approx_kl = (-logratio).mean()\n",
        "                approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
        "\n",
        "            mb_advantages = b_advantages_v[mb_inds]\n",
        "            if norm_adv:\n",
        "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "            # Policy loss\n",
        "            pg_loss1 = -mb_advantages * ratio\n",
        "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "            # Value loss\n",
        "            newvalue = newvalue.view(-1)\n",
        "            if clip_vloss:\n",
        "                v_loss_unclipped = (newvalue - b_returns_v[mb_inds]) ** 2\n",
        "                v_clipped = b_values_v[mb_inds] + torch.clamp(\n",
        "                    newvalue - b_values_v[mb_inds],\n",
        "                    -clip_coef,\n",
        "                    clip_coef,\n",
        "                )\n",
        "                v_loss_clipped = (v_clipped - b_returns_v[mb_inds]) ** 2\n",
        "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                v_loss = 0.5 * v_loss_max.mean()\n",
        "            else:\n",
        "                v_loss = 0.5 * ((newvalue - b_returns_v[mb_inds]) ** 2).mean()\n",
        "\n",
        "            entropy_loss = entropy.mean()\n",
        "            loss_value = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
        "\n",
        "            #get checkpoints before updating\n",
        "            if checkpoint:\n",
        "              if loss_value < loss:\n",
        "                agent_v.checkpoint(epoch)\n",
        "\n",
        "            loss = loss_value\n",
        "            optimizer_v.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(agent_v.parameters(), max_grad_norm)\n",
        "            optimizer_v.step()\n",
        "\n",
        "        if target_kl is not None:\n",
        "            if approx_kl > target_kl:\n",
        "                break\n",
        "        time.sleep(1)\n",
        "        bar()\n",
        "\n",
        "  return v_loss, pg_loss, entropy_loss, old_approx_kl, approx_kl, clipfracs, b_values_v, b_returns_v\n"
      ],
      "metadata": {
        "id": "NGcmHoTZFldu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nPcAQQgMFpRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}