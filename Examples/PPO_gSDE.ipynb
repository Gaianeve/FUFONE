{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example of how to integrate PPO with gSDE üìö"
      ],
      "metadata": {
        "id": "c7jyvLvlIEaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîñü¶Ñ Modify the Policy Network\n",
        "\n",
        "The policy network should output not only the parameters of the action distribution (e.g., mean and possibly variance for Gaussian actions) but also parameters for the state-dependent noise."
      ],
      "metadata": {
        "id": "fsIAf2ihIa1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dOxBAGaH0R-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as distributions\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim, 64)\n",
        "        self.mu = nn.Linear(64, action_dim)  # Mean of the action distribution\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Log standard deviation for Gaussian\n",
        "        self.state_dependent_noise = nn.Linear(state_dim, action_dim)  # Additional layer for state-dependent noise\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc(state))\n",
        "        mu = self.mu(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        noise = self.state_dependent_noise(state)\n",
        "        return mu, std, noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîñ üèÆ Implement State-Dependent Noise:\n",
        "\n",
        "Introduce a mechanism to generate noise based on the current state. This could involve an additional network or a module within the policy network that computes noise parameters (e.g., a noise covariance matrix).\n"
      ],
      "metadata": {
        "id": "B_xeQdNjIsP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_action(mu, std, noise, epsilon):\n",
        "    # Sample noise from standard normal distribution\n",
        "    noise_sample = epsilon * noise\n",
        "    # Apply state-dependent noise to the action\n",
        "    action = mu + std * noise_sample\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "cJc27TF5I78B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîñ ‚ú® Action Sampling with gSDE:\n",
        "\n",
        "Modify the action sampling process to incorporate the state-dependent noise. This means that for each state, you generate noise based on the state and add it to the action mean before sampling the final action."
      ],
      "metadata": {
        "id": "U885oRMWJAIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(policy_network, state):\n",
        "    mu, std, noise = policy_network(state)\n",
        "    epsilon = torch.randn_like(mu)  # Standard normal noise\n",
        "    action = sample_action(mu, std, noise, epsilon)\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "KTs5Nl_YJJrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîñüç•  Adapt the PPO Update:\n",
        "\n",
        "Ensure that the PPO update rules (the clipped surrogate objective) are applied to the new action sampling method. The PPO algorithm's core remains the same, but it now works with actions sampled using gSDE."
      ],
      "metadata": {
        "id": "Ngx1QWFjJNSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppo_update(policy_network, optimizer, states, actions, log_probs, returns, advantages, epsilon=0.2, beta=0.01):\n",
        "    mu, std, noise = policy_network(states)\n",
        "    new_log_probs = distributions.Normal(mu + noise, std).log_prob(actions).sum(axis=-1)\n",
        "    ratio = torch.exp(new_log_probs - log_probs)\n",
        "\n",
        "    # PPO objective with clipping\n",
        "    surrogate1 = ratio * advantages\n",
        "    surrogate2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
        "    policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "\n",
        "    # Entropy bonus to encourage exploration\n",
        "    entropy = distributions.Normal(mu, std).entropy().mean()\n",
        "    loss = policy_loss - beta * entropy\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "h1iJ5nNbJdF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}