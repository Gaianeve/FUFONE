{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example of how to use History Wrappers üìö"
      ],
      "metadata": {
        "id": "fgMyPDxfw1yM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Environment and Wrapper üéÅ\n",
        "Create your environment and wrap it using the HistoryWrapper.\n",
        "\n",
        "You need to use such an environment wrapper whenever you break Markov assumption.\n",
        "The main problem here is the ***continuity cost,*** here implemented with `weight=beta`, that's what breaks markov assumption.\n"
      ],
      "metadata": {
        "id": "cAWiqP20xAov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj2H0MWbwyVQ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from stable_baselines3 import PPO\n",
        "import numpy as np\n",
        "\n",
        "# Your HistoryWrapper class implementation here\n",
        "class HistoryWrapper(gym.Wrapper):\n",
        "    def __init__(self, env: gym.Env, steps: int, use_continuity_cost: bool):\n",
        "        super().__init__(env)\n",
        "        assert steps > 1, \"steps must be > 1\"\n",
        "        self.steps = steps\n",
        "        self.use_continuity_cost = use_continuity_cost\n",
        "        self.beta #weight of continuity cost\n",
        "\n",
        "        # concat obs with action\n",
        "        self.step_low = np.concatenate([env.observation_space.low, env.action_space.low])\n",
        "        self.step_high = np.concatenate([env.observation_space.high, env.action_space.high])\n",
        "\n",
        "        # stack for each step\n",
        "        obs_low = np.tile(self.step_low, (self.steps, 1))\n",
        "        obs_high = np.tile(self.step_high, (self.steps, 1))\n",
        "\n",
        "        self.observation_space = Box(low=obs_low.flatten(), high=obs_high.flatten(), dtype=np.float32)\n",
        "\n",
        "        self.history = self._make_history()\n",
        "\n",
        "    def _make_history(self):\n",
        "        return [np.zeros_like(self.step_low) for _ in range(self.steps)]\n",
        "\n",
        "    def _continuity_cost(self, obs):\n",
        "        continuity_cost = 0\n",
        "        for i in range(1, len(obs)):\n",
        "            action = obs[i][-len(self.env.action_space.low):]\n",
        "            last_action = obs[i-1][-len(self.env.action_space.low):]\n",
        "            continuity_cost += self.beta *np.sum(np.square(action - last_action))\n",
        "        return continuity_cost / (self.steps - 1)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.history.pop(0)\n",
        "\n",
        "        obs = np.concatenate([obs, action])\n",
        "        self.history.append(obs)\n",
        "        obs = np.array(self.history, dtype=np.float32)\n",
        "\n",
        "        if self.use_continuity_cost:\n",
        "            continuity_cost = self._continuity_cost(obs)\n",
        "            reward -= continuity_cost\n",
        "            info[\"continuity_cost\"] = continuity_cost\n",
        "\n",
        "        return obs.flatten(), reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        self.history = self._make_history()\n",
        "        self.history.pop(0)\n",
        "        obs, info = self.env.reset(seed=seed, options=options)\n",
        "        obs = np.concatenate([obs, np.zeros_like(self.env.action_space.low)])\n",
        "        self.history.append(obs)\n",
        "        return np.array(self.history, dtype=np.float32).flatten(), info\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "wrapped_env = HistoryWrapper(env, steps=4, use_continuity_cost=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with Stable Baselines3 üé•\n",
        "\n",
        "Use Stable Baselines3 to train a model with the wrapped environment."
      ],
      "metadata": {
        "id": "cZKtd13syQx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Define the RL model\n",
        "model = PPO('MlpPolicy', wrapped_env, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_history_cartpole\")\n"
      ],
      "metadata": {
        "id": "EzCKFifhyHPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation üèÉ\n",
        "Evaluate the trained model to see how it performs with the history wrapper.\n",
        "\n"
      ],
      "metadata": {
        "id": "E1CAczeayY6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = PPO.load(\"ppo_history_cartpole\")\n",
        "\n",
        "obs = wrapped_env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, reward, done, info = wrapped_env.step(action)\n",
        "    wrapped_env.render()\n",
        "    if done:\n",
        "        obs = wrapped_env.reset()\n",
        "\n",
        "wrapped_env.close()\n"
      ],
      "metadata": {
        "id": "DCwDZsnxyjKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}