# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Gaianeve/FUFONE/blob/main/PPO/Main.ipynb

# Simulated Fufi üê∂
Let's try to simulate our Cart-Pole, and train a model to control it. The algorithm to do that is the CleanRL PPO, explained at [this youtube video ](https://www.youtube.com/watch?v=MEt6rrxH8W4).

## W&B Setup
#ü™Ñ Install `wandb` library and login

Start by installing the library and logging in to your free account.
"""

#!pip install wandb -qU
# Log in to your W&B account
#import wandb
#wandb.login()

"""## Installing libraries üìö"""

# useful for import notebook
#!pip install import-ipynb
#!pip install gym==0.25.2
#needed from March
#!pip install numpy==1.23.5

"""## Setting things up for the environment üåç ü™ñ"""

import os

#saving current directory just to be sure
#content_dir = os.getcwd()

#cloning Fufi repo from git
#!git clone https://github.com/Gaianeve/gym-Fufi.git
#installing things
#!pip install /content/gym-Fufi

# Commented out IPython magic to ensure Python compatibility.
# Enter the environment directory
# %cd /content/gym-Fufi
# Actually importing the library for our environment
#import gym_Fufi

# Commented out IPython magic to ensure Python compatibility.
#get back to content directory so I save everything there
# %cd ..
#!pwd

"""## importing libraries and functions üìö

"""

#libraries
import argparse
import random
import time
from distutils.util import strtobool
import gym
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
from torch.distributions.categorical import Categorical
from torch.utils.tensorboard import SummaryWriter
from torchsummary import summary

"""## Log needed function from files üì°
Loading files directly from git, so I don't have to upload it by hand.
"""

#get files from git
#!git clone https://github.com/Gaianeve/FUFONE.git

"""### üê® In case you are in `content` directory and need to cancel the repo, just type:
```
import shutil
shutil.rmtree('FUFONE')
```



"""

#check which directory I'm in
#!pwd

# Commented out IPython magic to ensure Python compatibility.
#!pwd
# %cd FUFONE/PPO
from environment import vectorize_env
from agent_class import Agent
from agent_utils import anneal, collect_data, GAE, PPO_train_agent, evaluate_agent

# Commented out IPython magic to ensure Python compatibility.
#back to content directory
# %cd ..
# %cd ..

"""## PARSER ISSUE ü§î
declaring the parser here.

**Importing the parse_args() function from an another notebook doesn't work, so you need to define the function just ubove the main**

For what I have understood, the point is that ther's some problem with the notebook interface, because is user-friendly.
"""

def parse_args():
    # fmt: off
    parser = argparse.ArgumentParser()
    parser.add_argument("--exp-name", type=str, default= "Fufi_adventures",
        help="the name of this experiment")
    parser.add_argument("--gym-id", type=str, default="Fufi-v0",
        help="the id of the gym environment")
    parser.add_argument("--learning-rate", type=float, default=1.5e-4,
        help="the learning rate of the optimizer")
    parser.add_argument("--seed", type=int, default=1,
        help="seed of the experiment")
    parser.add_argument("--total-timesteps", type=int, default= 500000,
        help="total timesteps of the experiments")
    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="if toggled, `torch.backends.cudnn.deterministic=False`")
    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="if toggled, cuda will be enabled by default")
    #W&B setup
    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="if toggled, this experiment will be tracked with Weights and Biases")
    parser.add_argument("--wandb-project-name", type=str, default="Fufino",
        help="the wandb's project name")
    parser.add_argument("--wandb-entity", type=str, default=None,
        help="the entity (team) of wandb's project")
    parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="weather to capture videos of the agent performances (check out `videos` folder)")
    # Algorithm specific arguments
    parser.add_argument("--num-envs", type=int, default=8,
        help="the number of parallel game environments")
    parser.add_argument("--num-steps", type=int, default=512,
        help="the number of steps to run in each environment per policy rollout")
    parser.add_argument("--anneal-lr", type=lambda x: bool(strtobool(x)), default= True, nargs="?", const=True,
        help="Toggle learning rate annealing for policy and value networks")
    parser.add_argument("--gae", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="Use GAE for advantage computation")
    parser.add_argument("--gamma", type=float, default=0.99,
        help="the discount factor gamma")
    parser.add_argument("--gae-lambda", type=float, default=0.95,
        help="the lambda for the general advantage estimation")
    parser.add_argument("--num-minibatches", type=int, default=4,
        help="the number of mini-batches")
    parser.add_argument("--update-epochs", type=int, default=20,
        help="the K epochs to update the policy")
    parser.add_argument("--norm-adv", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="Toggles advantages normalization")
    parser.add_argument("--clip-coef", type=float, default=0.2,
        help="the surrogate clipping coefficient")
    parser.add_argument("--clip-vloss", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="Toggles whether or not to use a clipped loss for the value function, as per the paper.")
    parser.add_argument("--ent-coef", type=float, default=0.07,
        help="coefficient of the entropy")
    parser.add_argument("--vf-coef", type=float, default=0.5,
        help="coefficient of the value function")
    parser.add_argument("--max-grad-norm", type=float, default=0.5,
        help="the maximum norm for the gradient clipping")
    parser.add_argument("--target-kl", type=float, default=None,
        help="the target KL divergence threshold") #should be set to 0.015 if wanna use
    args, unknown = parser.parse_known_args()
    args.batch_size = int(args.num_envs * args.num_steps)
    args.minibatch_size = int(args.batch_size // args.num_minibatches)
    # fmt: on
    return args

"""## The instructions start from here üöÄ"""

def main():
## ----------------------------------------- PARSER ------------------------------------------------
  # retrieve the parser
  args = parse_args()    
  run_name = f"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}"

## -------------------------------------- W&B, TENSORBOARD ----------------------------------------

  # weight and biases flag
  if args.track:
      import wandb
      wandb.login()
      wandb.init(
          project=args.wandb_project_name,
          entity=args.wandb_entity,
          sync_tensorboard=True,
          config=vars(args),
          name=run_name,
          save_code=True,
      )

  # tensorboard setup
  writer = SummaryWriter(f"runs/{run_name}")
  writer.add_text(
      "hyperparameters",
      "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
  )

  ## ------------------------------------- SETTING UP THE GAME -------------------------------------------

  # TRY NOT TO MODIFY: seeding
  random.seed(args.seed)
  np.random.seed(args.seed)
  torch.manual_seed(args.seed)
  torch.backends.cudnn.deterministic = args.torch_deterministic

  device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")

  # env setup
  envs = vectorize_env(args.gym_id, args.seed, args.capture_video, run_name, args.num_envs)

  # Agent setup
  agent = Agent(envs).to(device)
  #agent.print_summary(envs)
  optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
  scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'max')

  # initializing things
  # ALGO Logic: Storage setup
  obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
  actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
  logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
  rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
  dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
  values = torch.zeros((args.num_steps, args.num_envs)).to(device)

## ------------------------------------- START THE GAME -------------------------------------------
  # TRY NOT TO MODIFY: start the game
  global_step = 0
  start_time = time.time()
  next_obs = torch.Tensor(envs.reset()).to(device)
  next_done = torch.zeros(args.num_envs).to(device)
  num_updates = args.total_timesteps // args.batch_size
    
  #sum of episodic returns
  sum_episodes = 0
  for update in range(1, num_updates + 1):
    #print('Starting update {}'.format(update))
    # Annealing the rate if instructed to do so.
    optimizer.param_groups[0]["lr"] = anneal(args.anneal_lr, update, num_updates, \
                                             args.learning_rate)

    for step in range(0, args.num_steps):
      # update global steps
      global_step += 1 * args.num_envs
      #update parameters
      obs, actions, logprobs, rewards, dones, values, next_obs, next_done, info \
      = collect_data(envs, obs, actions, logprobs, rewards, dones, values, next_obs,\
                     next_done, agent, step, device)

      # update tensorboard
      if 'episode' in info.keys():
        for item in info['episode']:
          if item is not None:
            #print(f"global_step={global_step}, episodic_return={item['r']}")
            writer.add_scalar("charts/episodic_return", item["r"], global_step)
            sum_episodes += item["r"]
            writer.add_scalar("charts/episodic_length", item["l"], global_step)

    # general advantages estimation
    returns, advantages = GAE(args.gae, args.gae_lambda, args.gamma, agent,\
        values, dones, rewards, next_obs, next_done,\
        args.num_steps, device)
    # flatten the batch
    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
    b_logprobs = logprobs.reshape(-1)
    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
    b_advantages = advantages.reshape(-1)
    b_returns = returns.reshape(-1)
    b_values = values.reshape(-1)

## ------------------------------------- TRAINING LOOP ----------------------------------------------
    v_loss, pg_loss, entropy_loss, old_approx_kl, approx_kl, clipfracs,\
    b_values, b_returns = PPO_train_agent(args.batch_size, args.update_epochs, args.minibatch_size, \
                                      args.clip_coef, args.norm_adv, args.clip_vloss,\
                                      args.ent_coef, args.vf_coef, args.max_grad_norm, args.target_kl,\
                                      agent, optimizer, scheduler, False,\
                                      b_obs, b_actions,b_logprobs,\
                                      b_advantages, b_returns, b_values)

## --------------------------------- UPDATING AND CLOSING UP -----------------------------------------
    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
    var_y = np.var(y_true)
    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y

    # TRY NOT TO MODIFY: record rewards for plotting purposes
    writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
    writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
    writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
    writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
    writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
    writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
    writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
    writer.add_scalar("losses/explained_variance", explained_var, global_step)
    #print("SPS:", int(global_step / (time.time() - start_time)))
    writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)


# log on W&B the sum of episodes returns
  wandb.log({"episodic_return_sum": sum_episodes}) 
## -------------------------------------- Log video to W&B ---------------------------------------------------
  if args.capture_video:
    video_files = [file for file in os.listdir(f"./videos/{run_name}") if file.endswith(".mp4")]
    video_files.sort(key=lambda x: os.path.getctime(os.path.join(f"./videos/{run_name}", x)))
    for video_file in video_files:
        #print(video_file)
        video_path = os.path.join(f"./videos/{run_name}", video_file)
        wandb.log({"episode_video": wandb.Video(video_path, fps=4, format="mp4")})
  envs.close()
  writer.close()
  wandb.finish()

"""## Tensorboard ‚ú®

"""

# Commented out IPython magic to ensure Python compatibility.

# %load_ext tensorboard
# %tensorboard --logdir runs

"""## Save the model if it's worthwhile ü¶∫
saved model goes into the directory `'content/models'`, by the name you give the function
"""

#agent.save_agent('Fufettino')

"""## Evaluation üôè üõï ‚õ© ‚õ™

"""

#mean_ev, std_ev = evaluate_agent(args.gym_id, args.seed, run_name, device, agent)
#print('evaluation test:')
#print(mean_ev)
#print(std_ev)

